# PPO-CartPole ğŸ§ ğŸ¢

This repo implements the **Proximal Policy Optimization (PPO)** algorithm using **PyTorch** to solve the classic **CartPole-v1** environment from OpenAI Gym. The agent learns to balance a pole on a moving cart using reinforcement learning principles.

---

## ğŸ“Œ Features

- ğŸš€ PPO algorithm with clipping
- ğŸ¯ Advantage estimation using **GAE**
- ğŸ§  Shared actor-critic neural network
- ğŸ“ˆ Tracks episode rewards and action distribution
- ğŸ§ª Compatible with Gymnasium and classic Gym

---

## ğŸ“š PPO Overview

**Proximal Policy Optimization (PPO)** is a policy gradient method that improves training stability by constraining how much the policy can change at each step. It uses:

- A **clipped surrogate objective** to prevent overly large policy updates.
- **Actor-Critic architecture**: shared features with separate policy and value heads.
- **GAE (Generalized Advantage Estimation)** for better advantage estimates.
- **Mini-batch updates** from collected trajectories.

---

## ğŸ› ï¸ Requirements

- `gym` or `gymnasium`
- `torch`
- `numpy`

Install with:

```bash
pip install gym torch numpy
